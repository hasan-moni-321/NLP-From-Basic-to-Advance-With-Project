{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "OY-mMXf4R9wM",
    "outputId": "6c8db49d-d267-4d1e-e028-a936ddd5794c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import string, re\n",
    "from unicodedata import normalize\n",
    "from numpy.random import shuffle\n",
    "from numpy import argmax\n",
    "from keras.models import load_model\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from pickle import load, dump\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Embedding, RepeatVector, TimeDistributed\n",
    "from keras.callbacks import ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s3_dLTu5SFzM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D1LsA2vqSKd1"
   },
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sXh1VOYRSF3I"
   },
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    file = open(filename, mode='rt', encoding='utf-8')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZXo85KPFSF5l"
   },
   "outputs": [],
   "source": [
    "filename = '/content/drive/My Drive/German English Translation/deu.txt'\n",
    "doc = load_doc(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JWycwtq6SF8J"
   },
   "outputs": [],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DpPrpf6sSGB8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pf0yWyQASVX1"
   },
   "source": [
    "## Document into Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TPV1_QYKSGLL"
   },
   "outputs": [],
   "source": [
    "def to_pairs(doc):\n",
    "    lines = doc.strip().split('\\n')\n",
    "    pairs = [line.split('\\t') for line in lines]\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mpo5g58hSGPs"
   },
   "outputs": [],
   "source": [
    "pairs = to_pairs(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "iBTvv3ELSGW7",
    "outputId": "7d2b67f1-5d05-42b2-9ef9-69a280159ba9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Go.', 'Geh.', 'CC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #8597805 (Roujin)']\n",
      "['Hi.', 'Hallo!', 'CC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #380701 (cburgmer)']\n",
      "['Hi.', 'Grüß Gott!', 'CC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #659813 (Esperantostern)']\n",
      "['Run!', 'Lauf!', 'CC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #941078 (Fingerhut)']\n",
      "['Run.', 'Lauf!', 'CC-BY 2.0 (France) Attribution: tatoeba.org #4008918 (JSakuragi) & #941078 (Fingerhut)']\n"
     ]
    }
   ],
   "source": [
    "#printing five list\n",
    "for i in range(5):\n",
    "  print(pairs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "OhkE_pSfSGdz",
    "outputId": "548264d1-e20d-482e-e2f4-8ce89feff72e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "208486"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0vC30PhwcK2m"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V8xuILyCSf_T"
   },
   "source": [
    "## Clean Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xz45C9ulSGlz"
   },
   "outputs": [],
   "source": [
    "# clean a list of lines\n",
    "def clean_pairs(lines):\n",
    "    cleaned = list()\n",
    "    # prepare regex for char filtering\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "    \n",
    "    for pair in lines:\n",
    "        clean_pair = list()\n",
    "        for line in pair:\n",
    "            \n",
    "            # normalize unicode characters\n",
    "            line = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "            line = line.decode('UTF-8')\n",
    "            line = line.split()\n",
    "            line = [word.lower() for word in line]\n",
    "            \n",
    "            # remove punctuation from each token\n",
    "            line = [re_punc.sub('', w) for w in line]\n",
    "            \n",
    "            # remove non-printable chars form each token\n",
    "            line = [re_print.sub('', w) for w in line]\n",
    "            line = [word for word in line if word.isalpha()]\n",
    "            # store as string\n",
    "            clean_pair.append(' '.join(line))\n",
    "        cleaned.append(clean_pair)\n",
    "    return np.array(cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "99FpR_zZSGpZ"
   },
   "outputs": [],
   "source": [
    "clean_pairs = clean_pairs(pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "colab_type": "code",
    "id": "xt93QoK9SGr0",
    "outputId": "f68d5cb1-e18d-4f0a-fd84-4abf6e6dc580"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['go', 'geh', 'ccby france attribution tatoebaorg cm roujin'],\n",
       "       ['hi', 'hallo', 'ccby france attribution tatoebaorg cm cburgmer'],\n",
       "       ['hi', 'gru gott',\n",
       "        'ccby france attribution tatoebaorg cm esperantostern'],\n",
       "       ...,\n",
       "       ['if someone who doesnt know your background says that you sound like a native speaker it means they probably noticed something about your speaking that made them realize you werent a native speaker in other words you dont really sound like a native speaker',\n",
       "        'wenn jemand der deine herkunft nicht kennt sagt dass du wie ein muttersprachler sprichst bedeutet das dass man wahrscheinlich etwas an deiner sprechweise bemerkt hat das erkennen lie dass du kein muttersprachler bist mit anderen worten du horst dich nicht wirklich wie ein muttersprachler an',\n",
       "        'ccby france attribution tatoebaorg ck tamy'],\n",
       "       ['if someone who doesnt know your background says that you sound like a native speaker it means they probably noticed something about your speaking that made them realize you werent a native speaker in other words you dont really sound like a native speaker',\n",
       "        'wenn jemand fremdes dir sagt dass du dich wie ein muttersprachler anhorst bedeutet das wahrscheinlich er hat etwas an deinem sprechen bemerkt dass dich als nichtmuttersprachler verraten hat mit anderen worten du horst dich nicht wirklich wie ein muttersprachler an',\n",
       "        'ccby france attribution tatoebaorg ck tickler'],\n",
       "       ['doubtless there exists in this world precisely the right woman for any given man to marry and vice versa but when you consider that a human being has the opportunity of being acquainted with only a few hundred people and out of the few hundred that there are but a dozen or less whom he knows intimately and out of the dozen one or two friends at most it will easily be seen when we remember the number of millions who inhabit this world that probably since the earth was created the right man has never yet met the right woman',\n",
       "        'ohne zweifel findet sich auf dieser welt zu jedem mann genau die richtige ehefrau und umgekehrt wenn man jedoch in betracht zieht dass ein mensch nur gelegenheit hat mit ein paar hundert anderen bekannt zu sein von denen ihm nur ein dutzend oder weniger nahesteht darunter hochstens ein oder zwei freunde dann erahnt man eingedenk der millionen einwohner dieser weltleicht dass seit erschaffung ebenderselben wohl noch nie der richtige mann der richtigen frau begegnet ist',\n",
       "        'ccby france attribution tatoebaorg rm pfirsichbaeumchen']],\n",
       "      dtype='<U527')"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PP67Sx8LSGvv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qwl7Utp8Ss3h"
   },
   "source": [
    "## Saving Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l5RCqfSNSGi7"
   },
   "outputs": [],
   "source": [
    "def save_clean_data(sentences, filename):\n",
    "    dump(sentences, open(filename, 'wb'))\n",
    "    print('Saved: %s' % filename)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "VkPq7-E_SGhs",
    "outputId": "1b4ad318-2e8c-4af4-d38d-fba0ba6319e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: english-german.pkl\n"
     ]
    }
   ],
   "source": [
    "save_clean_data(clean_pairs, 'english-german.pkl')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "46zs0lCycKUb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YwHft5CgcKvb"
   },
   "source": [
    "## Printing Some Pair of Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "wyssGhXZSGb7",
    "outputId": "85c25bc3-ab5c-4389-a2aa-9ddfa99c4f39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[go] => [geh]\n",
      "[hi] => [hallo]\n",
      "[hi] => [gru gott]\n",
      "[run] => [lauf]\n",
      "[run] => [lauf]\n",
      "[wow] => [potzdonner]\n",
      "[wow] => [donnerwetter]\n",
      "[fire] => [feuer]\n",
      "[help] => [hilfe]\n",
      "[help] => [zu hulf]\n",
      "[stop] => [stopp]\n",
      "[wait] => [warte]\n",
      "[wait] => [warte]\n",
      "[begin] => [fang an]\n",
      "[go on] => [mach weiter]\n",
      "[hello] => [hallo]\n",
      "[hurry] => [beeil dich]\n",
      "[hurry] => [schnell]\n",
      "[i ran] => [ich rannte]\n",
      "[i see] => [ich verstehe]\n",
      "[i see] => [aha]\n",
      "[i try] => [ich probiere es]\n",
      "[i won] => [ich hab gewonnen]\n",
      "[i won] => [ich habe gewonnen]\n",
      "[relax] => [entspann dich]\n",
      "[shoot] => [feuer]\n",
      "[shoot] => [schie]\n",
      "[smile] => [lacheln]\n",
      "[attack] => [angriff]\n",
      "[attack] => [attacke]\n",
      "[cheers] => [zum wohl]\n",
      "[eat it] => [iss es]\n",
      "[eat up] => [iss auf]\n",
      "[freeze] => [keine bewegung]\n",
      "[freeze] => [stehenbleiben]\n",
      "[got it] => [verstanden]\n",
      "[got it] => [aha]\n",
      "[got it] => [ich habs]\n",
      "[got it] => [kapiert]\n",
      "[got it] => [verstanden]\n",
      "[got it] => [einverstanden]\n",
      "[he ran] => [er rannte]\n",
      "[he ran] => [er lief]\n",
      "[hop in] => [mach mit]\n",
      "[hug me] => [druck mich]\n",
      "[hug me] => [nimm mich in den arm]\n",
      "[hug me] => [umarme mich]\n",
      "[i fell] => [ich fiel]\n",
      "[i fell] => [ich fiel hin]\n",
      "[i fell] => [ich sturzte]\n",
      "[i fell] => [ich bin hingefallen]\n",
      "[i fell] => [ich bin gesturzt]\n",
      "[i know] => [ich wei]\n",
      "[i lied] => [ich habe gelogen]\n",
      "[i lost] => [ich habe verloren]\n",
      "[i paid] => [ich habe bezahlt]\n",
      "[i paid] => [ich zahlte]\n",
      "[i sang] => [ich sang]\n",
      "[i swim] => [ich schwimme]\n",
      "[im] => [ich bin jahre alt]\n",
      "[im] => [ich bin]\n",
      "[im ok] => [mir gehts gut]\n",
      "[im ok] => [es geht mir gut]\n",
      "[im up] => [ich bin wach]\n",
      "[im up] => [ich bin auf]\n",
      "[no way] => [unmoglich]\n",
      "[no way] => [das kommt nicht in frage]\n",
      "[no way] => [das gibts doch nicht]\n",
      "[no way] => [ausgeschlossen]\n",
      "[no way] => [in keinster weise]\n",
      "[really] => [wirklich]\n",
      "[really] => [echt]\n",
      "[really] => [im ernst]\n",
      "[thanks] => [danke]\n",
      "[try it] => [versuchs]\n",
      "[we try] => [wir versuchen es]\n",
      "[we won] => [wir haben gewonnen]\n",
      "[why me] => [warum ich]\n",
      "[ask tom] => [frag tom]\n",
      "[ask tom] => [fragen sie tom]\n",
      "[ask tom] => [fragt tom]\n",
      "[awesome] => [fantastisch]\n",
      "[be cool] => [entspann dich]\n",
      "[be fair] => [sei nicht ungerecht]\n",
      "[be fair] => [sei fair]\n",
      "[be kind] => [sei nett]\n",
      "[be nice] => [sei nett]\n",
      "[be nice] => [seien sie nett]\n",
      "[beat it] => [geh weg]\n",
      "[beat it] => [hau ab]\n",
      "[beat it] => [verschwinde]\n",
      "[beat it] => [verdufte]\n",
      "[beat it] => [mach dich fort]\n",
      "[beat it] => [zieh leine]\n",
      "[beat it] => [mach dich vom acker]\n",
      "[beat it] => [verzieh dich]\n",
      "[beat it] => [verkrumele dich]\n",
      "[beat it] => [troll dich]\n",
      "[beat it] => [zisch ab]\n",
      "[beat it] => [pack dich]\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    print('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V6ykOI7lSGaS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Np_o_NJGS4xZ"
   },
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zvjmdoIiSGUT"
   },
   "outputs": [],
   "source": [
    "def load_clean_sentences(filename):\n",
    "    return load(open(filename, 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2Z6CVfA2SGSO"
   },
   "outputs": [],
   "source": [
    "raw_dataset = load_clean_sentences('english-german.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "colab_type": "code",
    "id": "83A5SjqKSGNl",
    "outputId": "89a1c800-43cb-4876-84eb-cbdf8040cc7a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['go', 'geh', 'ccby france attribution tatoebaorg cm roujin'],\n",
       "       ['hi', 'hallo', 'ccby france attribution tatoebaorg cm cburgmer'],\n",
       "       ['hi', 'gru gott',\n",
       "        'ccby france attribution tatoebaorg cm esperantostern'],\n",
       "       ...,\n",
       "       ['if someone who doesnt know your background says that you sound like a native speaker it means they probably noticed something about your speaking that made them realize you werent a native speaker in other words you dont really sound like a native speaker',\n",
       "        'wenn jemand der deine herkunft nicht kennt sagt dass du wie ein muttersprachler sprichst bedeutet das dass man wahrscheinlich etwas an deiner sprechweise bemerkt hat das erkennen lie dass du kein muttersprachler bist mit anderen worten du horst dich nicht wirklich wie ein muttersprachler an',\n",
       "        'ccby france attribution tatoebaorg ck tamy'],\n",
       "       ['if someone who doesnt know your background says that you sound like a native speaker it means they probably noticed something about your speaking that made them realize you werent a native speaker in other words you dont really sound like a native speaker',\n",
       "        'wenn jemand fremdes dir sagt dass du dich wie ein muttersprachler anhorst bedeutet das wahrscheinlich er hat etwas an deinem sprechen bemerkt dass dich als nichtmuttersprachler verraten hat mit anderen worten du horst dich nicht wirklich wie ein muttersprachler an',\n",
       "        'ccby france attribution tatoebaorg ck tickler'],\n",
       "       ['doubtless there exists in this world precisely the right woman for any given man to marry and vice versa but when you consider that a human being has the opportunity of being acquainted with only a few hundred people and out of the few hundred that there are but a dozen or less whom he knows intimately and out of the dozen one or two friends at most it will easily be seen when we remember the number of millions who inhabit this world that probably since the earth was created the right man has never yet met the right woman',\n",
       "        'ohne zweifel findet sich auf dieser welt zu jedem mann genau die richtige ehefrau und umgekehrt wenn man jedoch in betracht zieht dass ein mensch nur gelegenheit hat mit ein paar hundert anderen bekannt zu sein von denen ihm nur ein dutzend oder weniger nahesteht darunter hochstens ein oder zwei freunde dann erahnt man eingedenk der millionen einwohner dieser weltleicht dass seit erschaffung ebenderselben wohl noch nie der richtige mann der richtigen frau begegnet ist',\n",
       "        'ccby france attribution tatoebaorg rm pfirsichbaeumchen']],\n",
       "      dtype='<U527')"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UUkUKOWdTAf5"
   },
   "source": [
    "## Dividing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oEdlL2KhSGI3"
   },
   "outputs": [],
   "source": [
    "# reduce dataset size\n",
    "n_sentences = 20000\n",
    "dataset = raw_dataset[:n_sentences, :]\n",
    "# random shuffle\n",
    "shuffle(dataset)\n",
    "# split into train/test\n",
    "train, test = dataset[:18000], dataset[18000:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QKlUcSI3SGHL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BiEKk0RSTGQj"
   },
   "source": [
    "## Saving Divided dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_-GNb9xPSGF-"
   },
   "outputs": [],
   "source": [
    "# save a list of clean sentences to file\n",
    "def save_clean_data(sentences, filename):\n",
    "    dump(sentences, open(filename, 'wb'))\n",
    "    print('Saved: %s' % filename)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "DYX-3iLASF__",
    "outputId": "09164592-0abd-45f4-add1-3557d0ed2504"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: english-german-both.pkl\n",
      "Saved: english-german-train.pkl\n",
      "Saved: english-german-test.pkl\n"
     ]
    }
   ],
   "source": [
    "# save\n",
    "save_clean_data(dataset, 'english-german-both.pkl')\n",
    "save_clean_data(train, 'english-german-train.pkl')\n",
    "save_clean_data(test, 'english-german-test.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "edCuFSkpSF-s"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t4z60TG4TSGq"
   },
   "source": [
    "## Loading Divided dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ggz65ngJTP-I"
   },
   "outputs": [],
   "source": [
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "    return load(open(filename, 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K3Tjbr0eTQBA"
   },
   "outputs": [],
   "source": [
    "# load datasets\n",
    "dataset = load_clean_sentences('english-german-both.pkl')\n",
    "train = load_clean_sentences('english-german-train.pkl')\n",
    "test = load_clean_sentences('english-german-test.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W0CslmSXTQIS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0GcTf2ZITa5n"
   },
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Vke115GTQMt"
   },
   "outputs": [],
   "source": [
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dc3DWBmWTQQB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jCSy6HhKThGo"
   },
   "source": [
    "## Maximum sentence Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "85L-3WV_TQWu"
   },
   "outputs": [],
   "source": [
    "# max sentence length\n",
    "def max_length(lines):\n",
    "    return max(len(line.split()) for line in lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gWUMo3oATQZL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9p8-ylvhTp9Y"
   },
   "source": [
    "## English and German Tokenizer¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "_cVadM87TRM_",
    "outputId": "43f0c164-29cc-4420-86c6-e6ddf6f860fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 3640\n",
      "English Max Length: 5\n",
      "German Vocabulary Size: 5674\n",
      "German Max Length: 10\n"
     ]
    }
   ],
   "source": [
    "# prepare english tokenizer\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 0])\n",
    "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
    "print('English Max Length: %d' % (eng_length))\n",
    "\n",
    "# prepare german tokenizer\n",
    "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
    "ger_length = max_length(dataset[:, 1])\n",
    "print('German Vocabulary Size: %d' % ger_vocab_size)\n",
    "print('German Max Length: %d' % (ger_length))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-8iS6wECTRKu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KRgAs6gwTv5P"
   },
   "source": [
    "## Encoding and pad Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YWLhRiCRTRJo"
   },
   "outputs": [],
   "source": [
    "# encode and pad sequences\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "    # integer encode sequences\n",
    "    X = tokenizer.texts_to_sequences(lines)\n",
    "    # pad sequences with 0 values\n",
    "    X = pad_sequences(X, maxlen=length, padding='post')\n",
    "    return X\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3r499ZWMTQUM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4I2eGuOXT1oL"
   },
   "source": [
    "## One-hot-encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FGY6Ey0iTQSy"
   },
   "outputs": [],
   "source": [
    "# one hot encode target sequence\n",
    "def encode_output(sequences, vocab_size):\n",
    "    ylist = list()\n",
    "    for sequence in sequences:\n",
    "        encoded = to_categorical(sequence, num_classes=vocab_size)\n",
    "        ylist.append(encoded)\n",
    "    y = np.array(ylist)\n",
    "    y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mQ5KmURWTQLb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V0G3AvXLTQGM"
   },
   "outputs": [],
   "source": [
    "# prepare training data\n",
    "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
    "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
    "trainY = encode_output(trainY, eng_vocab_size)\n",
    "\n",
    "# prepare validation data\n",
    "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
    "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
    "testY = encode_output(testY, eng_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3ptChomGTQEz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KmH8rbRZUGK8"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0rG39SUKUEH2"
   },
   "outputs": [],
   "source": [
    "# define NMT model\n",
    "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
    "    model.add(LSTM(n_units))\n",
    "    model.add(RepeatVector(tar_timesteps))\n",
    "    model.add(LSTM(n_units, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
    "    \n",
    "    # compile model\n",
    "    model.compile(optimizer='adam', \n",
    "                  loss='categorical_crossentropy')\n",
    "    \n",
    "    # summarize defined model\n",
    "    model.summary()\n",
    "    plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "colab_type": "code",
    "id": "V48-tnF2em1X",
    "outputId": "4ef9aa3d-81ab-464b-e61b-c64bd7c3cc2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 10, 256)           1452544   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 5, 256)            0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 5, 256)            525312    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 5, 3640)           935480    \n",
      "=================================================================\n",
      "Total params: 3,438,648\n",
      "Trainable params: 3,438,648\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h5zEa68ZUELk"
   },
   "outputs": [],
   "source": [
    "#checkpoint\n",
    "checkpoint = ModelCheckpoint('model.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 746
    },
    "colab_type": "code",
    "id": "3qF3AGMlUEWA",
    "outputId": "4f417659-313c-4151-f72c-d464bc7798c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      " - 22s - loss: 0.0903 - val_loss: 2.2193\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.74147\n",
      "Epoch 2/10\n",
      " - 22s - loss: 0.0916 - val_loss: 2.1955\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.74147\n",
      "Epoch 3/10\n",
      " - 21s - loss: 0.0911 - val_loss: 2.2047\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.74147\n",
      "Epoch 4/10\n",
      " - 21s - loss: 0.0909 - val_loss: 2.2141\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.74147\n",
      "Epoch 5/10\n",
      " - 22s - loss: 0.0901 - val_loss: 2.2226\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.74147\n",
      "Epoch 6/10\n",
      " - 22s - loss: 0.0898 - val_loss: 2.2234\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.74147\n",
      "Epoch 7/10\n",
      " - 22s - loss: 0.0903 - val_loss: 2.2322\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.74147\n",
      "Epoch 8/10\n",
      " - 22s - loss: 0.0902 - val_loss: 2.2312\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.74147\n",
      "Epoch 9/10\n",
      " - 21s - loss: 0.0902 - val_loss: 2.2326\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.74147\n",
      "Epoch 10/10\n",
      " - 22s - loss: 0.0895 - val_loss: 2.2373\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.74147\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fd2fa59bb00>"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model.fit(trainX, trainY, epochs=10, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pgOOWKA6UEc-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a1ovB9SBUXdz"
   },
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dXVLcZunBMs7"
   },
   "source": [
    "##### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7XsMI4hjUEyU"
   },
   "outputs": [],
   "source": [
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "  return load(open(filename, 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H1rWsdFzAMmA"
   },
   "outputs": [],
   "source": [
    "# load datasets\n",
    "dataset = load_clean_sentences('english-german-both.pkl')\n",
    "train = load_clean_sentences('english-german-train.pkl')\n",
    "test = load_clean_sentences('english-german-test.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jfcK88y7hhSd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YX3GlTC9BUVC"
   },
   "source": [
    "##### Tokenizer and Max Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qvj9MlPQUEpc"
   },
   "outputs": [],
   "source": [
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "  tokenizer = Tokenizer()\n",
    "  tokenizer.fit_on_texts(lines)\n",
    "  return tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sCcsUOy7AbB8"
   },
   "outputs": [],
   "source": [
    "# max sentence length\n",
    "def max_length(lines):\n",
    "  return max(len(line.split()) for line in lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aM_MwHi6AX_K"
   },
   "outputs": [],
   "source": [
    "# prepare english tokenizer\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 0])\n",
    "\n",
    "# prepare german tokenizer\n",
    "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
    "ger_length = max_length(dataset[:, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OXiToumoUEnG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wjtPkky6Bgd6"
   },
   "source": [
    "##### Sequence Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lLnkidN9UElR"
   },
   "outputs": [],
   "source": [
    "# encode and pad sequences\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "  # integer encode sequences\n",
    "  X = tokenizer.texts_to_sequences(lines)\n",
    "  # pad sequences with 0 values\n",
    "  X = pad_sequences(X, maxlen=length, padding= 'post')\n",
    "  return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mUSgrk-IAkzB"
   },
   "outputs": [],
   "source": [
    "# prepare data\n",
    "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
    "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9lAWOMqtAljO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oaKBHyLxBn20"
   },
   "source": [
    "##### Generate Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yXUmyD5cUEgW"
   },
   "outputs": [],
   "source": [
    "# map an integer to a word\n",
    "def word_for_id(integer, tokenizer):\n",
    "  for word, index in tokenizer.word_index.items():\n",
    "    if index==integer:\n",
    "      return word\n",
    "  return None\n",
    "\n",
    "\n",
    "# generate target given source sequence\n",
    "def predict_sequence(model, tokenizer, source):\n",
    "  prediction = model.predict(source, verbose=0)[0]\n",
    "  integers = [argmax(vector) for vector in prediction]\n",
    "  target = list()\n",
    "  for i in integers:\n",
    "    word = word_for_id(i, tokenizer)\n",
    "    if word is None:\n",
    "      break\n",
    "    target.append(word)\n",
    "  return ' '.join(target)\n",
    "\n",
    "\n",
    "# evaluate the skill of the model\n",
    "def evaluate_model(model, sources, raw_dataset):\n",
    "  actual, predicted = list(), list()\n",
    "  for i, source in enumerate(sources):\n",
    "    # translate encoded source text\n",
    "    source = source.reshape((1, source.shape[0]))\n",
    "    translation = predict_sequence(model, eng_tokenizer, source)\n",
    "    raw_target, raw_src = raw_dataset[i,:2]\n",
    "    if i < 10:\n",
    "      print('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
    "    actual.append(raw_target.split())\n",
    "    predicted.append(translation.split())\n",
    "  \n",
    "  # calculate BLEU score\n",
    "  print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "  print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0))) \n",
    "  print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "  print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cvhdWwRKUESD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 662
    },
    "colab_type": "code",
    "id": "4VEIQIibUEQw",
    "outputId": "9e69efe2-9cbd-4863-bead-41cabfef5824"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "src=[tom brachte mich zum weinen], target=[tom made me cry], predicted=[tom made me cry]\n",
      "src=[konnt ihr das erledigen], target=[can you do this], predicted=[can you do that]\n",
      "src=[was fur eine gute idee], target=[what a nice idea], predicted=[what a good idea]\n",
      "src=[tom tanzt gerade], target=[tom is dancing], predicted=[tom is dancing]\n",
      "src=[er ist senil geworden], target=[hes gone senile], predicted=[hes gone senile]\n",
      "src=[tom hat sie nicht mehr alle], target=[tom is a psycho], predicted=[tom didnt us psycho]\n",
      "src=[wir sind beschaftigt tom], target=[were busy tom], predicted=[were all here]\n",
      "src=[er hat mich erpresst], target=[he blackmailed me], predicted=[he blackmailed me]\n",
      "src=[ich war zeuge], target=[i was a witness], predicted=[i was a witness]\n",
      "src=[ich hatte spa], target=[i had some fun], predicted=[i had fun fun]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.070148\n",
      "BLEU-2: 0.259870\n",
      "BLEU-3: 0.438786\n",
      "BLEU-4: 0.500180\n",
      "test\n",
      "src=[mein anzug ist grau], target=[my suit is grey], predicted=[my suit is gray]\n",
      "src=[ich bin auch beschaftigt], target=[im busy too], predicted=[im busy busy busy]\n",
      "src=[tom wich zuruck], target=[tom backed away], predicted=[tom backed out]\n",
      "src=[ich verstehe nicht wie], target=[i dont see how], predicted=[i dont not that]\n",
      "src=[ich bin nicht leichtglaubig], target=[im not gullible], predicted=[im not biased]\n",
      "src=[du bist sehr unhoflich], target=[youre very rude], predicted=[youre very very]\n",
      "src=[tom ist ein rupel], target=[tom is rude], predicted=[tom is a dwarf]\n",
      "src=[wo ist das auto], target=[wheres the car], predicted=[where the car]\n",
      "src=[das ist in ordnung], target=[thats fine], predicted=[thats ok]\n",
      "src=[unterrichten sie], target=[do you teach], predicted=[does you crying]\n",
      "BLEU-1: 0.065050\n",
      "BLEU-2: 0.249641\n",
      "BLEU-3: 0.427507\n",
      "BLEU-4: 0.489046\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "model = load_model('model.h5')\n",
    "\n",
    "# test on some training sequences\n",
    "print('train')\n",
    "evaluate_model(model, trainX, train)\n",
    "\n",
    "# test on some test sequences\n",
    "print('test')\n",
    "evaluate_model(model, testX, test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A02wk-lUUEO3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mzpLl0RLnT0g"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RqXZadecnT4e"
   },
   "source": [
    "##### Note:  How to improve/to get better accuracy\n",
    "\n",
    "1) Data cleaning\n",
    "\n",
    "2) Vocabulary --> reduce the less appeared of data\n",
    "\n",
    "3) Increase the number of total data\n",
    "\n",
    "4) Increase the epochs number\n",
    "\n",
    "5) Input Order. The order of input phrases could be reversed, which has been reported to\n",
    "    lift skill, or a Bidirectional input layer could be used.\n",
    "\n",
    "6) Increase the number of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "German to English Translation.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
